{"cells":[{"cell_type":"code","execution_count":null,"id":"a4b37af8-552d-4a44-ab7c-54c21ff7ea9a","metadata":{"id":"a4b37af8-552d-4a44-ab7c-54c21ff7ea9a"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import RobertaModel, RobertaTokenizer\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Custom Attention Layer in PyTorch\n","class CustomAttention(nn.Module):\n","    def __init__(self, input_dim):\n","        super(CustomAttention, self).__init__()\n","        self.W = nn.Parameter(torch.randn(input_dim, 1))\n","        self.b = nn.Parameter(torch.zeros(input_dim))\n","\n","    def forward(self, x):\n","        e = F.relu(torch.matmul(x, self.W) + self.b)\n","        a = torch.softmax(e, dim=1)\n","        output = torch.sum(x * a, dim=1)\n","        return output\n","\n","# Define the PyTorch Model\n","class MyModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(MyModel, self).__init__()\n","        self.roberta = RobertaModel.from_pretrained('roberta-base')\n","        self.cnn = nn.Conv1d(in_channels=768, out_channels=64, kernel_size=3, padding=1)\n","        self.attention = CustomAttention(input_dim=768)\n","        self.fc1 = nn.Linear(768 + 64, 256)\n","        self.fc2 = nn.Linear(256, num_classes)  # Change for multiclass\n","\n","    def forward(self, input_ids, attention_mask):\n","        roberta_output = self.roberta(input_ids, attention_mask=attention_mask)[0]\n","        cnn_output = self.cnn(roberta_output.transpose(1, 2))\n","        cnn_output = F.max_pool1d(cnn_output, kernel_size=cnn_output.shape[2]).squeeze(2)\n","        attention_output = self.attention(roberta_output)\n","        combined = torch.cat((cnn_output, attention_output), 1)\n","        x = F.relu(self.fc1(combined))\n","        x = self.fc2(x)  # No sigmoid activation\n","        return x\n","\n","# Data Preparation\n","class TweetDataset(Dataset):\n","    def __init__(self, tweets, labels, tokenizer, max_len):\n","        self.tweets = tweets\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.tweets)\n","\n","    def __getitem__(self, item):\n","        tweet = str(self.tweets[item])\n","        label = self.labels[item]\n","        encoding = self.tokenizer.encode_plus(\n","          tweet,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          truncation=True,\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","        )\n","        return {\n","          'tweet_text': tweet,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.long)  # Change for multiclass\n","        }\n","\n"]},{"cell_type":"code","execution_count":null,"id":"80211e3f-0e6a-40bf-b29a-b5a2eb045ec1","metadata":{"id":"80211e3f-0e6a-40bf-b29a-b5a2eb045ec1","outputId":"1172b403-561a-49cb-a19b-d16be05ef16e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Function to preprocess text data\n","def preprocess_text(text):\n","    # Implement text cleaning here (e.g., removing URLs, non-alphanumeric characters, etc.)\n","    return text\n","\n","# Load and preprocess data\n","df_train = pd.read_csv(\".../train2.csv\")\n","df_test = pd.read_csv(\".../test2.csv\")\n","\n","# Assuming preprocess_text function is defined\n","df_train['tweet'] = df_train['tweet'].apply(preprocess_text)\n","df_test['tweet'] = df_test['tweet'].apply(preprocess_text)\n","\n","# Encode labels for multiclass\n","# Manually re-number the dictionary values\n","encoded_dict = {\n","    \"admiration\": 0,\n","    \"appreciation\": 1,\n","    \"business\": 2,\n","    \"casualty\": 3,\n","    \"climate and environmental issues\": 4,\n","    \"communication\": 5,\n","    \"damage\": 6,\n","    \"die\": 7,\n","    \"disaster preparedness\": 8,\n","    \"education\": 9,\n","    \"empathy\": 10,\n","    \"health\": 11,\n","    \"humanitarian assistance\": 12,\n","    \"immigration\": 13,\n","    \"information dissemination\": 14,\n","    \"inquiry\": 15,\n","    \"life\": 16,\n","    \"memories\": 17,\n","    \"news\": 17,\n","    \"others\": 19,\n","    \"personal matters\": 20,\n","    \"politics\": 21,\n","    \"resources\": 22,\n","    \"safety\": 23,\n","    \"sport\": 24,\n","    \"spiritual\": 25,\n","    \"transportation\": 26,\n","    \"travel\": 27,\n","    \"warning\": 28\n","}\n","\n","df_train['event'] = df_train['event'].map(encoded_dict)\n","df_test['event'] = df_test['event'].map(encoded_dict)\n","\n","# Create datasets\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","train_dataset = TweetDataset(\n","    tweets=df_train.tweet.to_numpy(),\n","    labels=df_train.event.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=100\n",")\n","test_dataset = TweetDataset(\n","    tweets=df_test.tweet.to_numpy(),\n","    labels=df_test.event.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=100\n",")\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","# Set device to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Model Initialization and transfer to device\n","num_classes = len(encoded_dict)\n","model = MyModel(num_classes).to(device)\n","\n","# Optimizer and Loss Function\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n","loss_fn = nn.CrossEntropyLoss()  # Change for multiclass\n","\n"]},{"cell_type":"code","execution_count":null,"id":"fbc5ade7-8232-4c08-98cb-42a58f0e18c3","metadata":{"id":"fbc5ade7-8232-4c08-98cb-42a58f0e18c3","outputId":"66ce2bbc-537e-4f15-aa92-857093811898"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 1.0858, Accuracy: 0.7121\n","Epoch 2/10, Loss: 0.4206, Accuracy: 0.8748\n","Epoch 3/10, Loss: 0.2788, Accuracy: 0.9146\n","Epoch 4/10, Loss: 0.2032, Accuracy: 0.9371\n","Epoch 5/10, Loss: 0.1569, Accuracy: 0.9523\n","Epoch 6/10, Loss: 0.1231, Accuracy: 0.9613\n","Epoch 7/10, Loss: 0.1043, Accuracy: 0.9672\n","Epoch 8/10, Loss: 0.0849, Accuracy: 0.9725\n","Epoch 9/10, Loss: 0.0843, Accuracy: 0.9725\n","Epoch 10/10, Loss: 0.0725, Accuracy: 0.9758\n"]}],"source":["# Training Loop\n","num_epochs = 10  # Set the number of epochs\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_examples = 0\n","\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += (predicted == labels).sum().item()\n","        total_examples += labels.size(0)\n","\n","    avg_loss = total_loss / len(train_loader)\n","    accuracy = correct_predictions / total_examples\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n"]},{"cell_type":"code","execution_count":null,"id":"b914a6cd-7122-4789-b3dc-aa0a35a023a1","metadata":{"id":"b914a6cd-7122-4789-b3dc-aa0a35a023a1","outputId":"3e3abdfc-6c39-41e3-e095-10ec52281c02"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.99      1.00      1.00       137\n","           1       0.96      0.95      0.95       159\n","           2       1.00      0.99      0.99       302\n","           3       0.95      0.97      0.96       303\n","           4       0.99      1.00      0.99        97\n","           5       0.83      0.74      0.78        88\n","           6       0.87      0.81      0.84       411\n","           7       0.88      0.82      0.85       131\n","           8       0.85      0.78      0.81       447\n","           9       1.00      1.00      1.00       154\n","          10       0.85      0.74      0.79      1030\n","          11       0.99      1.00      1.00       145\n","          12       0.82      0.89      0.85       422\n","          13       0.97      0.98      0.98       124\n","          14       0.87      0.91      0.89      2110\n","          15       0.95      0.78      0.86        80\n","          16       1.00      1.00      1.00       106\n","          17       0.90      0.83      0.86       274\n","          19       0.55      0.39      0.46        85\n","          20       0.59      0.72      0.65       107\n","          21       0.97      0.98      0.97       114\n","          22       0.97      0.98      0.98       340\n","          23       0.84      0.88      0.86      1209\n","          24       0.99      1.00      1.00       204\n","          25       0.89      0.95      0.92       530\n","          26       1.00      1.00      1.00        71\n","          27       0.86      0.93      0.89       117\n","          28       1.00      0.99      0.99       228\n","\n","    accuracy                           0.89      9525\n","   macro avg       0.90      0.89      0.90      9525\n","weighted avg       0.89      0.89      0.89      9525\n","\n"]}],"source":["# Evaluation\n","model.eval()\n","predictions, true_labels = [], []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids, attention_mask)\n","        _, predicted = torch.max(outputs.data, 1)\n","        predictions.extend(predicted.cpu().tolist())\n","        true_labels.extend(labels.cpu().tolist())\n","\n","# Classification report and confusion matrix\n","print(classification_report(true_labels, predictions))\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}