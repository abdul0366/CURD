{"cells":[{"cell_type":"code","execution_count":null,"id":"8d70adfa-1e9e-4ed6-a7d4-5b22dcbf56d6","metadata":{"id":"8d70adfa-1e9e-4ed6-a7d4-5b22dcbf56d6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoModel, AutoTokenizer  # Use BERTweet Model and Tokenizer\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# Custom Attention Layer in PyTorch\n","class CustomAttention(nn.Module):\n","    def __init__(self, input_dim):\n","        super(CustomAttention, self).__init__()\n","        self.W = nn.Parameter(torch.randn(input_dim, 1))\n","        self.b = nn.Parameter(torch.zeros(input_dim))\n","\n","    def forward(self, x):\n","        e = F.relu(torch.matmul(x, self.W) + self.b)\n","        a = torch.softmax(e, dim=1)\n","        output = torch.sum(x * a, dim=1)\n","        return output\n","\n","# Define the PyTorch Model (using BERTweet)\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.bertweet = AutoModel.from_pretrained('vinai/bertweet-base')  # Using BERTweet base model\n","        self.cnn = nn.Conv1d(in_channels=768, out_channels=64, kernel_size=3, padding=1)\n","        self.attention = CustomAttention(input_dim=768)\n","        self.fc1 = nn.Linear(768 + 64, 256)\n","        self.fc2 = nn.Linear(256, 1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        bertweet_output = self.bertweet(input_ids, attention_mask=attention_mask)[0]\n","        cnn_output = self.cnn(bertweet_output.transpose(1, 2))\n","        cnn_output = F.max_pool1d(cnn_output, kernel_size=cnn_output.shape[2]).squeeze(2)\n","        attention_output = self.attention(bertweet_output)\n","        combined = torch.cat((cnn_output, attention_output), 1)\n","        x = F.relu(self.fc1(combined))\n","        x = torch.sigmoid(self.fc2(x)).squeeze(1)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c60a3c23-4de0-4845-a60a-f6f7b0c76e1e","metadata":{"id":"c60a3c23-4de0-4845-a60a-f6f7b0c76e1e"},"outputs":[],"source":["\n","# Data Preparation\n","class TweetDataset(Dataset):\n","    def __init__(self, tweets, labels, tokenizer, max_len):\n","        self.tweets = tweets\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.tweets)\n","\n","    def __getitem__(self, item):\n","        tweet = str(self.tweets[item])\n","        label = self.labels[item]\n","        encoding = self.tokenizer.encode_plus(\n","          tweet,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          truncation=True,\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","        )\n","        return {\n","          'tweet_text': tweet,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.float)\n","        }\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e1a321a5-f896-494c-be12-51b75fcb7925","metadata":{"id":"e1a321a5-f896-494c-be12-51b75fcb7925","outputId":"f24f8230-fc6b-4393-d61a-f7228783fbb2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Function to preprocess text data\n","def preprocess_text(text):\n","    # Implement text cleaning here (e.g., removing URLs, non-alphanumeric characters, etc.)\n","    return text\n","\n","# Load and preprocess data\n","df_train = pd.read_csv(\".../Dataset/train_data2.csv\")\n","df_test = pd.read_csv(\".../Dataset/test_data2.csv\")\n","\n","# Assuming preprocess_text function is defined\n","df_train['tweet'] = df_train['no_stop_joined'].apply(preprocess_text)\n","df_test['tweet'] = df_test['no_stop_joined'].apply(preprocess_text)\n","\n","# Encode labels as binary\n","encoded_dict = {\"Relevant\": 0, \"Not Relevant\": 1}\n","df_train['event'] = df_train['Relevance'].map(encoded_dict)\n","df_test['event'] = df_test['Relevance'].map(encoded_dict)\n","\n","# Create datasets\n","tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')  # Using BERTweet tokenizer\n","train_dataset = TweetDataset(\n","    tweets=df_train.tweet.to_numpy(),\n","    labels=df_train.event.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=100\n",")\n","test_dataset = TweetDataset(\n","    tweets=df_test.tweet.to_numpy(),\n","    labels=df_test.event.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=100\n",")\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ba1f8ac0-5064-4930-8b24-f9d1c879f310","metadata":{"id":"ba1f8ac0-5064-4930-8b24-f9d1c879f310","outputId":"b9326440-00f1-45a5-bc01-149a24f3dac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["# Set device to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Model Initialization and transfer to device\n","model = MyModel().to(device)\n","\n","# Optimizer and Loss Function\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n","loss_fn = nn.BCELoss()"]},{"cell_type":"code","execution_count":null,"id":"d03fd149-feeb-4ad1-9d9c-aa0e87bc9870","metadata":{"id":"d03fd149-feeb-4ad1-9d9c-aa0e87bc9870","outputId":"97b4268f-2fd0-43c7-99da-e971218572f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10 - Loss: 0.3524, Accuracy: 0.8461\n","Epoch 2/10 - Loss: 0.2204, Accuracy: 0.9095\n","Epoch 3/10 - Loss: 0.1657, Accuracy: 0.9369\n","Epoch 4/10 - Loss: 0.1180, Accuracy: 0.9558\n","Epoch 5/10 - Loss: 0.0861, Accuracy: 0.9696\n","Epoch 6/10 - Loss: 0.0684, Accuracy: 0.9759\n","Epoch 7/10 - Loss: 0.0537, Accuracy: 0.9812\n","Epoch 8/10 - Loss: 0.0476, Accuracy: 0.9833\n","Epoch 9/10 - Loss: 0.0382, Accuracy: 0.9867\n","Epoch 10/10 - Loss: 0.0417, Accuracy: 0.9853\n"]}],"source":["from sklearn.metrics import accuracy_score\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass\n","        outputs = model(input_ids, attention_mask)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Aggregate loss\n","        total_loss += loss.item()\n","\n","        # Calculate accuracy\n","        predictions = (outputs > 0.5).float()  # Convert to binary predictions\n","        correct_predictions += (predictions == labels).sum().item()\n","        total_predictions += labels.size(0)\n","\n","    # Calculate average loss and accuracy over the epoch\n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = correct_predictions / total_predictions\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"8a38cb39-b5f4-4ebd-b4e6-27c725fa06a1","metadata":{"id":"8a38cb39-b5f4-4ebd-b4e6-27c725fa06a1"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","import torch\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","predictions, true_labels = [], []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids, attention_mask)\n","        predictions.extend(outputs.cpu().tolist())\n","        true_labels.extend(labels.cpu().tolist())\n","\n","# Convert predictions to binary\n","binary_predictions = [1 if p > 0.5 else 0 for p in predictions]\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"bfa2ede2-a069-4553-9505-a1c90caa846b","metadata":{"id":"bfa2ede2-a069-4553-9505-a1c90caa846b","outputId":"36e71569-0d91-4332-9fd1-a1caa3cae45d"},"outputs":[{"name":"stdout","output_type":"stream","text":["AUC-ROC: 0.9547216609889879\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.94      0.94      3808\n","         1.0       0.83      0.82      0.83      1411\n","\n","    accuracy                           0.91      5219\n","   macro avg       0.88      0.88      0.88      5219\n","weighted avg       0.91      0.91      0.91      5219\n","\n","AUC-ROC: 0.9547216609889879\n"]}],"source":["# Assuming predictions are single probability values for the positive class\n","auc_roc = roc_auc_score(true_labels, predictions)\n","print(f'AUC-ROC: {auc_roc}')\n","\n","# Classification report and confusion matrix\n","print(classification_report(true_labels, binary_predictions))\n","print(f'AUC-ROC: {auc_roc}')\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}